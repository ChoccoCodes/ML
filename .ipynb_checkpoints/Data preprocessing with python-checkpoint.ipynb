{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing (Scaling) with Python.\n",
    "\n",
    "Data preprocesing is the act of transforming data into more useful forms.\n",
    "The main data preprocessing methods include scaling, normalization, binarization and standardization.\n",
    "In today's case, we will use a three python libraries to illustrate data preprocessing:\n",
    "1. Pandas(we will use one of its modules to open the csv file we are going to use).\n",
    "2. Sklearn(has a couple of useful modules that we will use).\n",
    "3. Numpy(for its set_printoptions module).\n",
    "\n",
    "*I have used the terms modules and methods interchangeably, just saying.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling\n",
    "\n",
    "from pandas import read_csv\n",
    "from numpy import set_printoptions\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'pima-indians-diabetes.csv'\n",
    "dataframe = read_csv(path)\n",
    "array = dataframe.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization refers to rescaling real-valued numeric attributes into a 0 to 1 range. Data normalization is used in machine learning to make model training less sensitive to the scale of features. This allows our model to converge to better weights and, in turn, leads to a more accurate model. To learn more about data normalization visit, https://www.educative.io/answers/data-normalization-in-python#:~:text=Normalization%20refers%20to%20rescaling%20real,to%20a%20more%20accurate%20model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.05882353 0.42713568 0.54098361 ... 0.11656704 0.16666667 0.        ]\n",
      " [0.47058824 0.91959799 0.52459016 ... 0.25362938 0.18333333 1.        ]\n",
      " [0.05882353 0.44723618 0.54098361 ... 0.03800171 0.         0.        ]\n",
      " ...\n",
      " [0.29411765 0.6080402  0.59016393 ... 0.07130658 0.15       0.        ]\n",
      " [0.05882353 0.63316583 0.49180328 ... 0.11571307 0.43333333 1.        ]\n",
      " [0.05882353 0.46733668 0.57377049 ... 0.10119556 0.03333333 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#Use the MinMax Scaler to rescale the data to the range of 0 to 1.as_integer_ratio.\n",
    "#This is data normalization.\n",
    "data_scaler = preprocessing.MinMaxScaler(feature_range = (0,1))\n",
    "data_rescaled = data_scaler.fit_transform(array)\n",
    "\n",
    "print(data_rescaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second method of data scaling is Binarization. As the name suggests, it is the conversion of data into two states usually denoted by 0 and 1. In our case, a person whose diabetes results is positive in our data set, his value will translate to a 1 andthe diabetes-free individual will evaluate as 0 after the data is binarized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. ... 0. 1. 0.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 0. 1. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 1. 0.]\n",
      " [1. 1. 1. ... 0. 1. 1.]\n",
      " [1. 1. 1. ... 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import Binarizer\n",
    "path = 'pima-indians-diabetes.csv'\n",
    "df = read_csv(path)\n",
    "array = df.values\n",
    "binarizer = Binarizer(threshold = 0.5).fit(array)\n",
    "Data_binarized = binarizer.transform(array)\n",
    "\n",
    "print(Data_binarized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.84372629 -1.12208597 -0.16024856 ... -0.36426474 -0.18894038\n",
      "  -0.73075304]\n",
      " [ 1.23423997  1.94447577 -0.26357823 ...  0.60470064 -0.1037951\n",
      "   1.36845138]\n",
      " [-0.84372629 -0.99692019 -0.16024856 ... -0.91968415 -1.0403932\n",
      "  -0.73075304]\n",
      " ...\n",
      " [ 0.343683    0.0044061   0.14974046 ... -0.68423462 -0.27408566\n",
      "  -0.73075304]\n",
      " [-0.84372629  0.16086333 -0.47023757 ... -0.37030191  1.17338414\n",
      "   1.36845138]\n",
      " [-0.84372629 -0.8717544   0.04641078 ... -0.47293375 -0.87010264\n",
      "  -0.73075304]]\n"
     ]
    }
   ],
   "source": [
    "#Standardization(Gaussian Distribution)\n",
    "# Using the standard scaler class.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "data_scaler = StandardScaler().fit(array)\n",
    "data_rescaled = data_scaler.transform(array)\n",
    "\n",
    "print(data_rescaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "FEATURE SELECTION TECHNIQUES AND WHY FEATURE SELECTION IS IMPORTANT.\n",
    "Importance of feature selection.\n",
    "1. Helps prevent overfitting in models.\n",
    "2. Will increase efficiency of the model.\n",
    "3. Reduce training time.\n",
    "4. Help boost generalization of models.\n",
    "5. Minimize collinearity while enhancing interpretability.\n",
    "6. Helps avoid the hectiness of dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Univariate selection.\n",
    "from pandas import read_csv\n",
    "from numpy import set_printoptions\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 110.73 1406.59   17.5    51.01 2219.4   127.67    5.36  178.01]\n",
      "\n",
      " Featured Data: \n",
      " [[ 85.    0.   26.6  31. ]\n",
      " [183.    0.   23.3  32. ]\n",
      " [ 89.   94.   28.1  21. ]\n",
      " [137.  168.   43.1  33. ]]\n"
     ]
    }
   ],
   "source": [
    "path = 'pima-indians-diabetes.csv'\n",
    "df = read_csv(path)\n",
    "array = df.values\n",
    "\n",
    "#Separate array into input and output components.\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "test = SelectKBest(score_func = chi2, k=4)\n",
    "fit = test.fit(X,Y)\n",
    "set_printoptions(precision = 2)\n",
    "print(fit.scores_)\n",
    "\n",
    "featured_data = fit.transform(X)\n",
    "print(\"\\n Featured Data: \\n\", featured_data[0:4])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d535c1a89bc2325fb7e4e0b65b381789fada57f120444c0dba78f05ecce83dee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
