{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing (Scaling) with Python.\n",
    "\n",
    "Data preprocesing is the act of transforming data into more useful forms.\n",
    "The main data preprocessing methods include scaling, normalization, binarization and standardization.\n",
    "In today's case, we will use a three python libraries to illustrate data preprocessing:\n",
    "1. Pandas(we will use one of its modules to open the csv file we are going to use).\n",
    "2. Sklearn(has a couple of useful modules that we will use).\n",
    "3. Numpy(for its set_printoptions module).\n",
    "\n",
    "*I have used the terms modules and methods interchangeably, just saying.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling\n",
    "\n",
    "from pandas import read_csv\n",
    "from numpy import set_printoptions\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'pima-indians-diabetes.csv'\n",
    "dataframe = read_csv(path)\n",
    "array = dataframe.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization refers to rescaling real-valued numeric attributes into a 0 to 1 range. Data normalization is used in machine learning to make model training less sensitive to the scale of features. This allows our model to converge to better weights and, in turn, leads to a more accurate model. To learn more about data normalization visit, https://www.educative.io/answers/data-normalization-in-python#:~:text=Normalization%20refers%20to%20rescaling%20real,to%20a%20more%20accurate%20model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.05882353 0.42713568 0.54098361 ... 0.11656704 0.16666667 0.        ]\n",
      " [0.47058824 0.91959799 0.52459016 ... 0.25362938 0.18333333 1.        ]\n",
      " [0.05882353 0.44723618 0.54098361 ... 0.03800171 0.         0.        ]\n",
      " ...\n",
      " [0.29411765 0.6080402  0.59016393 ... 0.07130658 0.15       0.        ]\n",
      " [0.05882353 0.63316583 0.49180328 ... 0.11571307 0.43333333 1.        ]\n",
      " [0.05882353 0.46733668 0.57377049 ... 0.10119556 0.03333333 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#Use the MinMax Scaler to rescale the data to the range of 0 to 1.as_integer_ratio.\n",
    "#This is data normalization.\n",
    "data_scaler = preprocessing.MinMaxScaler(feature_range = (0,1))\n",
    "data_rescaled = data_scaler.fit_transform(array)\n",
    "\n",
    "print(data_rescaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second method of data scaling is Binarization. As the name suggests, it is the conversion of data into two states usually denoted by 0 and 1. In our case, a person whose diabetes results is positive in our data set, his value will translate to a 1 andthe diabetes-free individual will evaluate as 0 after the data is binarized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. ... 0. 1. 0.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 0. 1. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 1. 0.]\n",
      " [1. 1. 1. ... 0. 1. 1.]\n",
      " [1. 1. 1. ... 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import Binarizer\n",
    "path = 'pima-indians-diabetes.csv'\n",
    "df = read_csv(path)\n",
    "array = df.values\n",
    "binarizer = Binarizer(threshold = 0.5).fit(array)\n",
    "Data_binarized = binarizer.transform(array)\n",
    "\n",
    "print(Data_binarized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.84372629 -1.12208597 -0.16024856 ... -0.36426474 -0.18894038\n",
      "  -0.73075304]\n",
      " [ 1.23423997  1.94447577 -0.26357823 ...  0.60470064 -0.1037951\n",
      "   1.36845138]\n",
      " [-0.84372629 -0.99692019 -0.16024856 ... -0.91968415 -1.0403932\n",
      "  -0.73075304]\n",
      " ...\n",
      " [ 0.343683    0.0044061   0.14974046 ... -0.68423462 -0.27408566\n",
      "  -0.73075304]\n",
      " [-0.84372629  0.16086333 -0.47023757 ... -0.37030191  1.17338414\n",
      "   1.36845138]\n",
      " [-0.84372629 -0.8717544   0.04641078 ... -0.47293375 -0.87010264\n",
      "  -0.73075304]]\n"
     ]
    }
   ],
   "source": [
    "#Standardization(Gaussian Distribution)\n",
    "# Using the standard scaler class.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "data_scaler = StandardScaler().fit(array)\n",
    "data_rescaled = data_scaler.transform(array)\n",
    "\n",
    "print(data_rescaled)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e9720426f39ecd062449cc775873164c87e93521332a2ff5fc5cd859efbaef6d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
